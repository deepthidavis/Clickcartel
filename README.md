# ğŸ“¦ Clickcartel â€“ End-to-End Spark Data Engineering Pipeline

## ğŸš€ Project Overview

You are a Data Engineer at **Clickcartel**, a rapidly growing e-commerce platform.  
The marketing and product teams struggle to make data-driven decisions because:

- User event data is messy  
- Customer & product data is disconnected  
- No unified analytics layer exists  

They approach you with a critical business need:

> **â€œWe need to understand our user journey.  
What products are people viewing?  
What actions lead to purchases?  
Who are our most valuable customers?â€**

Your mission:

### âœ”ï¸ Build an automated, scalable **multi-layered Spark/Delta Lake pipeline**  
âœ”ï¸ Process raw user event streams  
âœ”ï¸ Clean & enrich data with customer/product information  
âœ”ï¸ Produce aggregated **Gold** tables for analytics  

This pipeline becomes Clickcartelâ€™s **single source of truth** for user behavior.

---

## ğŸ“Š Data Sources (Synthetic Generation)

### **1ï¸âƒ£ Raw User Events (JSON, Streaming)**
- Ingested via **Structured Streaming (Auto Loader)**
- Contains:
  - `timestamp`
  - `user_id`
  - `event_type` â†’ `view_product`, `add_to_cart`, `purchase`
  - `product_id`
- **Intentionally skewed** so certain products receive disproportionately high number of views.

### **2ï¸âƒ£ Customer Profiles (CSV, Batch)**
- Dimension table with:
  - `customer_id`
  - `signup_date`
  - `location`

### **3ï¸âƒ£ Product Details (Parquet, Batch)**
- Product catalog with:
  - `product_id`
  - `product_name`
  - `category`
  - `price`

---

# ğŸ§± Section 1: Apache Spark Architecture & Components

This project demonstrates Spark fundamentals:

- **Execution hierarchy** â†’ jobs, stages, tasks  
- **Lazy evaluation** â†’ transformations build the DAG before actions  
- Modules used:
  - Spark SQL  
  - DataFrames / Dataset API  
  - Structured Streaming  

---

# ğŸ§  Section 2: Spark SQL Concepts Used

### âœ”ï¸ Reading multiple formats
- JSON (Auto Loader)  
- CSV  
- Parquet  

### âœ”ï¸ Writing Delta tables
- `append` for streaming  
- `overwrite` for batch refresh  

### âœ”ï¸ Partitioning
- Gold tables use `partitionBy()` for optimized query performance.

---

# ğŸ› ï¸ Section 3: Spark DataFrame / Dataset API Techniques

### ğŸ”¹ Column Manipulation
- `withColumn()`  
- `withColumnRenamed()`  
- `select()`  

### ğŸ”¹ Deduplication
- Stream-to-stream deduplication using:
  ```python
  withWatermark("event_time", "10 minutes").dropDuplicates(["event_id"])
